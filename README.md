#  ðŸš¢ Titanic Survival Prediction â€” Endâ€‘toâ€‘End ML Project

A polished, hands-on machine learning pipeline that predicts passenger survival on the Titanic using the classic Kaggle dataset. Designed for clarity, reproducibility, and fast iterationâ€”with clean preprocessing, focused feature engineering, and model evaluation.

# âœ¨ Highlights

1.) Endâ€‘toâ€‘end workflow: load â†’ preprocess â†’ engineer â†’ train â†’ evaluate â†’ predict

2.) Clean handling of missing values and categorical encoding without leakage

3.)Baseline and improved models with accuracy and F1 tracking

4.)Reproducible notebook structure and exportable predictions

# ðŸ§° Tech Stack

Language: Python 3.x

Libraries: NumPy, pandas, scikitâ€‘learn, seaborn, matplotlib


# ðŸ“¦ Dataset

Source: Kaggle Titanic (train.csv, test.csv)

Target: Survived (0/1)

Core features used:

   Numerical: Pclass, Age, SibSp, Parch, Fare
   
   Categorical: Sex, Embarked
   
   Typically dropped/noisy: Name, Ticket, Cabin (can derive features)


# ðŸ“ Project Structure


titanic-ml/
â”œâ”€ Titanic-Project.ipynb        # Main notebook: EDA â†’ preprocessing â†’ modeling â†’ outputs
â”œâ”€ data/
â”‚  â”œâ”€ train.csv
â”‚  â””â”€ test.csv
â”œâ”€ outputs/
â”‚  â””â”€ submission.csv            # Generated predictions (optional)
â”œâ”€ requirements.txt             # Dependencies (optional)
â””â”€ README.md                    # This file

# ðŸ”Ž Methodology

-> Data Loading & Sanity Checks

-> Inspect shapes, dtypes, and missingness patterns.

->Validate target distribution and key feature ranges.

->Missing Value Strategy

->Age â†’ median (or model-based) imputation

->Embarked â†’ most frequent category

->Fare (test) â†’ median if missing

->Cabin â†’ excluded in baseline; optional HasCabin flag

->Feature Engineering

   Sex â†’ binary encoding (male=1, female=0)
    
   Embarked â†’ oneâ€‘hot (Q, S; with appropriate reference handling)
   
   Numerical features retained: Pclass, Age, SibSp, Parch, Fare


# Modeling

Baseline: Logistic Regression or RandomForestClassifier

Metrics: Accuracy (primary), F1 (classâ€‘imbalance aware)

Selection: Choose model by CV performance and validation stability

Inference & Submission

Retrain best model on full training data

Predict on test set

Save submission.csv with PassengerId and Survived

# ðŸ“Š Metrics & Expectations

Primary: Accuracy

Secondary: F1â€‘score (macro/weighted)

Typical outcomes:

Logistic Regression â†’ strong, interpretable baseline

Treeâ€‘based models â†’ higher potential with tuned hyperparams

Note: Exact scores vary by features, seeds, and preprocessing choices; rely on CV for stability.
Exported predictions align PassengerId correctly with Survived

# ðŸ‘©ðŸ’» Author
Shreya Sourabh

Interests: Machine Learning, Data Science, applied modeling, deployment

Collaboration welcomeâ€”issues and PRs encouraged
